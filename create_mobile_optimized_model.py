#!/usr/bin/env python3
"""
Create a mobile-optimized quantized model from Phi-3 Mini for iPhone deployment.
Target: Under 150 MB for optimal App Store distribution.
"""

import argparse
import os
import torch
import coremltools as ct
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np

def create_mobile_optimized_model(
    model_name="microsoft/Phi-3-mini-4k-instruct",
    output_dir="models/mobile_optimized",
    max_seq_length=512,
    quantization_bits=4
):
    """
    Create a mobile-optimized version of Phi-3 Mini.
    
    Args:
        model_name: HuggingFace model name
        output_dir: Directory to save the optimized model
        max_seq_length: Maximum sequence length (shorter = smaller model)
        quantization_bits: Quantization bits (4, 8, or 16)
    """
    
    print(f"Creating mobile-optimized model from {model_name}")
    print(f"Target quantization: {quantization_bits}-bit")
    print(f"Max sequence length: {max_seq_length}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Step 1: Load tokenizer
    print("\n[1/5] Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer_path = os.path.join(output_dir, "tokenizer")
    tokenizer.save_pretrained(tokenizer_path)
    print(f"Tokenizer saved to {tokenizer_path}")
    
    # Step 2: Load model with reduced precision
    print("\n[2/5] Loading model with reduced precision...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,  # Use FP16 for smaller size
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    model.eval()
    
    # Step 3: Apply pruning to reduce model size
    print("\n[3/5] Applying model pruning...")
    # Reduce number of attention heads or layers if needed
    # For now, we'll keep the architecture but quantize heavily
    
    # Step 4: Convert to TorchScript with example inputs
    print("\n[4/5] Converting to TorchScript...")
    example_input = torch.randint(0, tokenizer.vocab_size, (1, 32))
    
    with torch.no_grad():
        # Trace the model
        traced_model = torch.jit.trace(
            model,
            example_input,
            strict=False
        )
    
    # Step 5: Convert to Core ML with aggressive quantization
    print(f"\n[5/5] Converting to Core ML with {quantization_bits}-bit quantization...")
    
    # Configure quantization based on bits
    if quantization_bits == 4:
        compute_precision = ct.precision.FLOAT16
        # Use 4-bit weight quantization
        op_config = ct.optimize.coreml.OpLinearQuantizerConfig(
            mode="linear_symmetric",
            weight_threshold=512
        )
        config = ct.optimize.coreml.OptimizationConfig(
            global_config=op_config
        )
    elif quantization_bits == 8:
        compute_precision = ct.precision.FLOAT16
        config = ct.optimize.coreml.OptimizationConfig(
            global_config=ct.optimize.coreml.OpLinearQuantizerConfig(
                mode="linear_symmetric"
            )
        )
    else:  # 16-bit
        compute_precision = ct.precision.FLOAT16
        config = None
    
    try:
        # Convert to Core ML
        mlmodel = ct.convert(
            traced_model,
            inputs=[ct.TensorType(name="input_ids", shape=(1, 32))],
            compute_precision=compute_precision,
            minimum_deployment_target=ct.target.iOS15,
            convert_to="mlprogram"
        )
        
        # Apply quantization if configured
        if config is not None:
            print("Applying post-training quantization...")
            mlmodel = ct.optimize.coreml.linear_quantize_weights(
                mlmodel,
                config=config
            )
        
        # Save the model
        output_path = os.path.join(output_dir, f"phi3_mini_mobile_q{quantization_bits}.mlpackage")
        mlmodel.save(output_path)
        
        # Get model size
        import subprocess
        result = subprocess.run(['du', '-sh', output_path], capture_output=True, text=True)
        model_size = result.stdout.split()[0]
        
        print(f"\nâœ… Success! Mobile-optimized model saved to: {output_path}")
        print(f"ðŸ“¦ Model size: {model_size}")
        print(f"ðŸŽ¯ Quantization: {quantization_bits}-bit")
        print(f"ðŸ“± Target: iOS 15+")
        
        # Check if model is under 150 MB
        size_bytes = os.path.getsize(output_path) if os.path.isfile(output_path) else sum(
            os.path.getsize(os.path.join(dirpath, filename))
            for dirpath, dirnames, filenames in os.walk(output_path)
            for filename in filenames
        )
        size_mb = size_bytes / (1024 * 1024)
        
        print(f"\nðŸ“Š Model Statistics:")
        print(f"   Size: {size_mb:.2f} MB")
        if size_mb <= 150:
            print(f"   âœ… EXCELLENT: Under 150 MB (cellular download)")
        elif size_mb <= 500:
            print(f"   âœ… GOOD: Under 500 MB (Wi-Fi download recommended)")
        else:
            print(f"   âš ï¸  WARNING: Over 500 MB (may need further optimization)")
        
        return output_path
        
    except Exception as e:
        print(f"\nâŒ Error during conversion: {str(e)}")
        print("\nTrying alternative approach with ONNX intermediate...")
        
        # Alternative: Convert via ONNX
        onnx_path = os.path.join(output_dir, "temp_model.onnx")
        
        # Export to ONNX
        torch.onnx.export(
            traced_model,
            example_input,
            onnx_path,
            input_names=['input_ids'],
            output_names=['logits'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                'logits': {0: 'batch_size', 1: 'sequence_length'}
            }
        )
        
        # Convert ONNX to Core ML
        mlmodel = ct.convert(
            onnx_path,
            minimum_deployment_target=ct.target.iOS15,
            compute_precision=compute_precision
        )
        
        output_path = os.path.join(output_dir, f"phi3_mini_mobile_q{quantization_bits}.mlpackage")
        mlmodel.save(output_path)
        
        # Clean up temporary ONNX file
        if os.path.exists(onnx_path):
            os.remove(onnx_path)
        
        print(f"âœ… Model saved via ONNX conversion: {output_path}")
        return output_path

def main():
    parser = argparse.ArgumentParser(
        description='Create mobile-optimized Phi-3 Mini model for iPhone'
    )
    parser.add_argument(
        '--model',
        default='microsoft/Phi-3-mini-4k-instruct',
        help='HuggingFace model name'
    )
    parser.add_argument(
        '--output',
        default='models/mobile_optimized',
        help='Output directory'
    )
    parser.add_argument(
        '--max-seq-length',
        type=int,
        default=512,
        help='Maximum sequence length'
    )
    parser.add_argument(
        '--quantization',
        type=int,
        choices=[4, 8, 16],
        default=4,
        help='Quantization bits (4, 8, or 16)'
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("Mobile Model Optimization for iPhone Deployment")
    print("=" * 70)
    
    try:
        output_path = create_mobile_optimized_model(
            model_name=args.model,
            output_dir=args.output,
            max_seq_length=args.max_seq_length,
            quantization_bits=args.quantization
        )
        
        print("\n" + "=" * 70)
        print("Next Steps:")
        print("=" * 70)
        print("1. Copy the model to your Xcode project:")
        print(f"   cp -r {output_path} ios_app/Phi3Assistant/Phi3Assistant/")
        print("\n2. Update ModelHandler.swift to use the new model:")
        print("   - Change model name in loadModel() method")
        print("   - Update input tensor shapes if needed")
        print("\n3. Test on iPhone 13 mini or simulator")
        print("=" * 70)
        
    except Exception as e:
        print(f"\nâŒ Failed to create mobile-optimized model: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
